(window.webpackJsonp=window.webpackJsonp||[]).push([[43],{452:function(t,a,o){"use strict";o.r(a);var s=o(2),r=Object(s.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"一、gpt-4o-简介"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#一、gpt-4o-简介"}},[t._v("#")]),t._v(" 一、GPT-4o 简介")]),t._v(" "),a("p",[t._v("北京时间 5 月 14 日，OpenAI 举行春季发布会。OpenAI 在活动中发布了新旗舰模型“GPT-4o”！据 OpenAI 首席技术官穆里·穆拉蒂（Muri Murati）介绍，GPT-4o 在继承 GPT-4 强大智能的同时，进一步提升了文本、图像及语音处理能力，为用户带来更加流畅、自然的交互体验，更多详情可点击查看"),a("a",{attrs:{href:"https://openai.com/index/hello-gpt-4o/",target:"_blank",rel:"noopener noreferrer"}},[t._v("官网"),a("OutboundLink")],1),t._v("。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gpt-4.oss-cn-hangzhou.aliyuncs.com/img/202405141436838.png",alt:"img"}})]),t._v(" "),a("p",[t._v("GPT-4o 的“o”代表“omni”，源自拉丁语“omnis”。在英语中“omni”常被用作词根，用来表示“全部”或“所有”的概念。GPT-4o 是一个多模态大模型，支持文本、音频和图像的任意组合输入，并能生成文本、音频和图像的任意组合输出。与现有模型相比，它在视觉和音频理解方面尤其出色。")]),t._v(" "),a("h2",{attrs:{id:"二、gpt-4o-的性能"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#二、gpt-4o-的性能"}},[t._v("#")]),t._v(" 二、GPT-4o 的性能")]),t._v(" "),a("p",[t._v("GPT-4o 可以在音频、视觉和文本中进行实时推理，接受文本、音频和图像的任何组合作为输入，并生成文本、音频和图像的任何组合进行输出。它可以最短在 232 毫秒内响应音频输入，平均为 320 毫秒，这与人类在对话中的响应时间相似。")]),t._v(" "),a("h3",{attrs:{id:"文本能力"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#文本能力"}},[t._v("#")]),t._v(" 文本能力")]),t._v(" "),a("p",[t._v("GPT-4o 在 0 次 COT MMLU（常识问题）上创下了 88.7% 的新高分。所有这些评估都是用我们的新 简单评估（在新窗口中打开）。此外，在传统的 5 次无 CoT MMLU 上，GPT-4o 创下了 87.2% 的新高。（注：3400b（在新窗口中打开）还在训练）")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gpt-4.oss-cn-hangzhou.aliyuncs.com/img/202405141436249.png",alt:"img"}})]),t._v(" "),a("h3",{attrs:{id:"音频能力"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#音频能力"}},[t._v("#")]),t._v(" 音频能力")]),t._v(" "),a("p",[t._v("GPT-4o 在语音翻译方面创下了新的领先地位，并在 MLS 基准测试中优于 Whisper-v3。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gpt-4.oss-cn-hangzhou.aliyuncs.com/img/202405141436218.png",alt:"img"}})]),t._v(" "),a("h3",{attrs:{id:"各种语言的考试能力"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#各种语言的考试能力"}},[t._v("#")]),t._v(" 各种语言的考试能力")]),t._v(" "),a("p",[t._v("M3Exam 基准测试既是多语言评估，也是视觉评估，由来自其他国家标准化考试的多项选择题组成，有时包括数字和图表。GPT-4o 在所有语言的基准测试中都比 GPT-4 强。（我们省略了斯瓦希里语和爪哇语的视力结果，因为这些语言只有 5 个或更少的视力问题。）")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gpt-4.oss-cn-hangzhou.aliyuncs.com/img/202405141436227.png",alt:"img"}})]),t._v(" "),a("h3",{attrs:{id:"视觉理解"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#视觉理解"}},[t._v("#")]),t._v(" 视觉理解")]),t._v(" "),a("p",[t._v("GPT-4o 在视觉感知基准测试中实现了最先进的性能。全面碾压之前的模型。所有视觉评估均为 0 次，其中 MMMU、MathVista 和 ChartQA 为 0 次 CoT。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gpt-4.oss-cn-hangzhou.aliyuncs.com/img/202405141436209.png",alt:"img"}})]),t._v(" "),a("h3",{attrs:{id:"语音交互"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#语音交互"}},[t._v("#")]),t._v(" 语音交互")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gpt-4.oss-cn-hangzhou.aliyuncs.com/img/202405141436578.png",alt:"img"}})]),t._v(" "),a("p",[t._v("GPT-4o 在语音交互方面取得了重大进展。它采用了先进技术，显著提高了响应速度，使得对话更加流畅自然。在最近的发布会上，OpenAI 展示了 GPT-4o 在语音对话中的表现，它能够几乎实时地回答问题，并通过文本转语音技术进行朗读，提供了一种沉浸式的交流体验。此外，GPT-4o 还可以调整说话的语气，从夸张戏剧到冰冷机械，以适应不同的交流场景。令人兴奋的是，GPT-4o 还具备唱歌的功能，增添了更多的趣味性和娱乐性。")]),t._v(" "),a("h2",{attrs:{id:"三、gpt-4-turbo-与-gpt-4o"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#三、gpt-4-turbo-与-gpt-4o"}},[t._v("#")]),t._v(" 三、GPT-4 Turbo 与 GPT-4o")]),t._v(" "),a("p",[t._v("不仅在传统的文本能力上 GPT-4 Turbo 的性能相当，还在 API 方面更快速，价格还更便宜 50%。总结来说，与 GPT-4 Turbo 相比，GPT-4o 速度提高了 2 倍，价格减半，限制速率提高了 5 倍。截至 2024 年 5 月 13 日，Plus 用户将能够在 GPT-4o 上每 3 小时发送多达 80 条消息，在 GPT-4 上每 3 小时发送多达 40 条消息。我们可能会在高峰时段减少限制，以保持 GPT-4 和 GPT-4o 可供尽可能多的人访问。")]),t._v(" "),a("p",[t._v("GPT-4o 具有相同的高智商，但比 GPT-4 Turbo 更快、更便宜，并且具有更高的速率限制。")]),t._v(" "),a("p",[t._v("具体说来：")]),t._v(" "),a("ul",[a("li",[t._v("定价：GPT-4o 比 GPT-4 Turbo 便宜 50%，输入 5 美元/月，输出代币 15 美元/M）。")]),t._v(" "),a("li",[t._v("速率限制：GPT-4o 的速率限制比 GPT-4 Turbo 高 5 倍——每分钟最多 1000 万个代币。")]),t._v(" "),a("li",[t._v("速度：GPT-4o 的速度是 GPT-2 Turbo 的 4 倍。")]),t._v(" "),a("li",[t._v("视觉：GPT-4o 的视觉能力在与视觉能力相关的评估中表现优于 GPT-4 Turbo。")]),t._v(" "),a("li",[t._v("多语言：GPT-4o 改进了对非英语语言的支持，而不是 GPT-4 Turbo。")])]),t._v(" "),a("p",[t._v("GPT-4o 目前的上下文窗口为 128k，知识截止日期为 2023 年 10 月。")]),t._v(" "),a("h2",{attrs:{id:"四、怎么使用-gpt-4o"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#四、怎么使用-gpt-4o"}},[t._v("#")]),t._v(" 四、怎么使用 GPT-4o"),a("img",{attrs:{src:"https://gpt-4.oss-cn-hangzhou.aliyuncs.com/img/202405141436197.png",alt:"img"}})]),t._v(" "),a("p",[t._v("目前，GPT-4o 的文本和图像功能已经开始在 ChatGPT 中逐步推出，用户可以在 ChatGPT 平台上免费体验到 GPT-4o 的相关功能，但免费版有使用次数限制，Plus 用户可以享受到 5 倍的调用额度（升级 plus 详细教程："),a("a",{attrs:{href:"https://chatgptzhanghao.github.io/how-to-register-a-chatgpt-account-in-china/",target:"_blank",rel:"noopener noreferrer"}},[t._v("升级 PLUS"),a("OutboundLink")],1),t._v("）。")]),t._v(" "),a("p",[t._v("在接下来的几周内，OpenAI 计划将在 ChatGPT Plus 中推出 Voice Mode 新版本，该版本带有 GPT-4o。这将作为 ChatGPT Plus 的一个 alpha 版本提供给 PIus 用户。此外，GPT-4o 也将通过 API 提供给开发者，作为文本和视觉模型。开发者可以利用 AP 来集成 GPT-4o 到他们自己的应用程序")]),t._v(" "),a("p",[t._v("中，而且 GPT-4o 在 API 中相比 GPT-4Tubo 更快、更便宜，并且有更高的速率限制。")]),t._v(" "),a("p",[t._v("至于 GPT-4o 的音频和视频功能，OpenAl 将在未来的几周和几个月内继续开发技术基础设施、通过训练后提高可用性以及确保安全性，之后才会发布这些功能，并逐步向公众提供。")])])}),[],!1,null,null,null);a.default=r.exports}}]);